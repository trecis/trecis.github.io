Run,date,team,description,paper,code,nDCG@100,Info-Type F1 [Actionable],Info-Type F1 [All],Info-Type Accuracy,Priority F1 [Actionable],Priority F1 [All],Priority R [Actionable],Priority R [All] 
deberta-mtl-fta,2021/10/14,ucd-cs,The run of deberta-base multi-task learning with few-shot augmentations,,,0.4630,0.2159,0.2647,0.9016üèÜ,0.2497,0.2361,0.2779,0.2834
njit_augly,2021/07/19,njit,NJIT RoBERTa run with AugLy in place of synonym augmentation.,,,0.4242,0.2441,0.2729,0.8811,0.1723,0.1565,0.1700,0.2172
STrans-GaussianNB,2021/10/10,ucd-cs,The run of using sentence transformers as the fixed features and use GaussianNB as the downstream classifier,,,0.3368,0.2083,0.2575,0.8474,0.1959,0.1712,0.1096,0.1417
njit_label_prop,2021/07/01,njit,"Model uses GPT2 to generate tweets and label propagation to label them, using which we train a new transformer model.",,,0.4381,0.2008,0.2600,0.8964,0.1719,0.1557,0.2087,0.2431
ens,2021/06/29,ucd-cs,"The ensemble run of multi-task learning with Deberta and eda augmentation, i.e., ucd-cs-run4 at 2021a (alignment issue fixed)",,https://github.com/wangcongcong123/crisis-mtl,0.4617,0.2670,0.2923,0.8685,0.2817,0.2623,0.2886,0.3182üèÜ
njit_debly,2021/10/15,njit,UCD ensemble run with different pipelines for information class classification and priority scoring and  augmentation with AugLy.,,,0.4070,0.2046,0.2502,0.8994,0.2323,0.1777,0.1608,0.2007
njit_semi_sup,2021/06/27,njit,"Model uses GPT2 to generate tweets and semi-supervision to label them, using which we train a new transformer model.",,,0.4385,0.2275,0.2780,0.9003,0.1719,0.1557,0.2087,0.2431
db-mtl-fta-nla,2021/10/18,ucd-cs,The run of deberta-base multi-task learning with few-shot augmentations plus noise label annealing,,,0.4193,0.1936,0.2488,0.8907,0.2727,0.2609,0.2650,0.2339
ens,2021/10/18,ucd-cs,UCD ensemble run with extra steps,,,0.4643,0.2784üèÜ,0.2946üèÜ,0.8728,0.2864üèÜ,0.2734üèÜ,0.2748,0.2827
njit_semi_sup,2021/07/06,njit,Semi-supervised generation pipeline with priority scores using highest average score of information type labels.,,,0.4219,0.2008,0.2600,0.8964,0.2271,0.2323,0.0849,0.1346
ens-fta,2021/10/18,ucd-cs,UCD ensemble run with extra steps plus few-shot augmentation and noise label annealing,,,0.4515,0.1131,0.2170,0.8073,0.2852,0.2724,0.2479,0.2665
njit_roberta,2021/06/23,njit,"Model that uses a simple text augmentation strategy for expanding training data. Then, we use a pre-trained RoBERTa model to generate text embeddings of tweets and classify them.",,,0.4394,0.2501,0.2753,0.8746,0.1719,0.1557,0.2087,0.2431
njit_deberta,2021/08/02,njit,NJIT RoBERTa run with DeBERTa instead of RoBERTa.,,,0.4541,0.1710,0.2312,0.7820,0.1610,0.1466,0.2587,0.3072
l3i_ttxth,2021/09/28,l3i,"INFO-PRIORITY-roBERTa-base+2xTransformer+event_type+event_title, 256, hashtags",,,0.4877,0.1645,0.2368,0.9003,0.2086,0.2293,0.3072üèÜ,0.3053
njit_label_prop,2021/07/08,njit,Label Propagation generation pipeline with priority scores using highest average score of information type labels.,,,0.4215,0.2008,0.2600,0.8964,0.2268,0.2288,0.1058,0.1225
l3i_ttxth_combined,2021/09/28,l3i,"INFO-roBERTa-base+2xTransformer+event_type+event_title, 256, hashtags, PRIORITY-roBERTa-base+2xTransformer+event_type+event_title, 280",,,0.4888,0.1928,0.2675,0.9004,0.2086,0.2293,0.3072,0.3053
njit_eda,2021/07/23,njit,NJIT RoBERTa run with Easy Data Augmentation (EDA) in place of synonym augmentation.,,,0.4292,0.2531,0.2735,0.8892,0.1647,0.1531,0.1553,0.2258
njit_roberta,2021/06/01,njit,,,,0.4385,0.2501,0.2753,0.8746,0.1719,0.1557,0.2087,0.2431
njit_random_run,2021/06/01,njit,,,,0.2485,0.0000,0.0044,0.8853,0.0444,0.1043,-0.0041,0.0024
RB_2T_TTH_256_LR5,2021/06/01,rb,,,,0.2892,0.1645,0.2368,0.9003,0.2364,0.2294,0.1487,0.1034
RB_2T_TT_280_SVM,2021/06/01,rb,,,,0.2333,0.1928,0.2675,0.9004,0.0695,0.1151,0.0009,-0.0085
RB_2T_MT_H_280,2021/06/01,rb,,,,0.4549,0.2271,0.2498,0.8999,0.2059,0.2177,0.2258,0.2515
ucd-cs-run1,2021/06/01,ucd-cs,,,,0.4727,0.2433,0.2772,0.8926,0.2657,0.2632,0.2590,0.2888
uogTr-04-coocc,2021/06/01,uofgtr,,,,0.3095,0.1556,0.2587,0.8937,0.0588,0.1035,0.1131,0.0908
njit_bert,2021/06/01,njit,,,,0.4102,0.1662,0.2111,0.8223,0.1480,0.1366,0.1057,0.2126
uogTr-01-pw,2021/06/01,uofgtr,,,,0.3094,0.0869,0.1798,0.8887,0.0588,0.1035,0.1131,0.0908
uogTr-02-pwcoocc,2021/06/01,uofgtr,,,,0.3098,0.1556,0.2606,0.8936,0.0588,0.1035,0.1131,0.0908
RB_2Tx2_TTH_280,2021/06/01,rb,,,,0.4904üèÜ,0.2289,0.2686,0.8949,0.2247,0.2352,0.3020,0.2773
njit_random_run,2021/06/01,njit,,,,0.2521,0.0452,0.1132,0.5002,0.2252,0.2044,0.0151,0.0122
ucd-cs-run2,2021/06/01,ucd-cs,,,,0.4569,0.2326,0.2753,0.8911,0.2536,0.2524,0.1995,0.2686
ucd-cs-run3,2021/06/01,ucd-cs,,,,0.4707,0.2538,0.2860,0.8930,0.2530,0.2694,0.2741,0.3053
ucd-cs-run4,2021/06/01,ucd-cs,,,,0.4620,0.0181,0.0532,0.7944,0.2817,0.2623,0.2886,0.3182
sienaRun,2021/06/01,siena,,,,0.4061,0.1788,0.2691,0.8607,0.1328,0.1722,0.0796,0.1128
ucr_use_1,2022/03/08,ucr-cse,Universal Sentence Encoder based model with optimized thresholding,,,0.4941üèÜ,0.1942,0.1767,0.8887,0.1010,0.1565,0.0891,0.1177
2021a-bert-base-uncased-single,2022/03/25,shu-1014,Ensemble Pretrained Language Models with Selective Attention,,,0.5597üèÜ,0.0915,0.1707,0.9031üèÜ,0.2165,0.2244,0.4414üèÜ,0.3856üèÜ
